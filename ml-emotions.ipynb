{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: click in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\menno\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menno\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\menno\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\menno\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\menno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\menno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\menno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\menno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\menno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')       # For sentence and word tokenization\n",
    "nltk.download('wordnet')     # For accessing WordNet\n",
    "nltk.download('omw-1.4')    # Open Multilingual Wordnet (needed for some WordNet functions)\n",
    "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging\n",
    "nltk.download('punkt_tab')  # Download punkt_tab resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheets: ['NRC-Emotion-Lexicon-v0.92-InMan']\n",
      "Using sheet: NRC-Emotion-Lexicon-v0.92-InMan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\menno\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Lexicon Size: 14181\n",
      "Sample Lexicon Size: 100\n",
      "Expanded Lexicon Size: 521\n",
      "Text 1 Scores: {'positive': 0, 'negative': 0, 'anger': 0, 'anticipation': 0, 'disgust': 0, 'fear': 0, 'joy': 0, 'sadness': 0, 'surprise': 0, 'trust': 0}\n",
      "Text 2 Scores: {'positive': 0, 'negative': 0, 'anger': 0, 'anticipation': 0, 'disgust': 0, 'fear': 0, 'joy': 0, 'sadness': 0, 'surprise': 0, 'trust': 0}\n",
      "Text 3 Scores: {'positive': 0, 'negative': 1, 'anger': 0, 'anticipation': 0, 'disgust': 1, 'fear': 1, 'joy': 0, 'sadness': 0, 'surprise': 0, 'trust': 0}\n",
      "       word  positive  negative  anger  anticipation  disgust  fear  joy  \\\n",
      "5627  grief         0         1      0             0        0     0    0   \n",
      "\n",
      "      sadness  surprise  trust  \n",
      "5627        1         0      0  \n",
      "Empty DataFrame\n",
      "Columns: [word, positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# --- Step 2: Load and Inspect the Emotion Lexicon ---\n",
    "\n",
    "def load_nrc_lexicon(file_path=None):\n",
    "    \"\"\"Loads the NRC Emotion Lexicon from a local file and returns a DataFrame.\n",
    "       Handles both XLSX and TSV/CSV formats.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        raise ValueError(\"Must provide a file path.\")\n",
    "\n",
    "    try:\n",
    "        # Load from a local file\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            print(\"Available sheets:\", excel_file.sheet_names)\n",
    "            sheet_name = excel_file.sheet_names[0] # Get the *actual* first sheet name\n",
    "            print(f\"Using sheet: {sheet_name}\")\n",
    "            df = excel_file.parse(sheet_name)\n",
    "        elif file_path.endswith('.txt') or file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath_or_buffer=file_path,sep='\\t', header=0) # Added header=0 for CSV with header row\n",
    "\n",
    "            if len(df.columns) < 11:\n",
    "              print(f\"Warning the file has an invalid amount of columns: {len(df.columns)} expected at least 11\")\n",
    "              return None\n",
    "        else:\n",
    "            print(\"Unsupported file format.  Please provide a .xlsx or .tsv/.csv file.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        # --- Common Preprocessing (after loading) ---\n",
    "\n",
    "        # 1. Handle inconsistent column names (KEY FIX)\n",
    "        # We'll rename the columns to a consistent set, *regardless* of\n",
    "        # whether they have spaces, parentheses, etc.\n",
    "        column_mapping = {\n",
    "            'English (en)': 'word',\n",
    "            'English': 'word',  # Handle case where it's just 'English'\n",
    "            'English Word': 'word',  # ***CORRECT MAPPING***\n",
    "             # Add other variations if needed, based on Step 1 output\n",
    "            'Positive': 'positive',\n",
    "            'Negative': 'negative',\n",
    "            'Anger': 'anger',\n",
    "            'Anticipation': 'anticipation',\n",
    "            'Disgust': 'disgust',\n",
    "            'Fear': 'fear',\n",
    "            'Joy': 'joy',\n",
    "            'Sadness': 'sadness',\n",
    "            'Surprise': 'surprise',\n",
    "            'Trust': 'trust'\n",
    "        }\n",
    "\n",
    "        # Rename columns, only if they exist in the DataFrame\n",
    "        for original, new in column_mapping.items():\n",
    "            if original in df.columns:\n",
    "                df = df.rename(columns={original: new})\n",
    "\n",
    "        # 2. Filter for English Words (if the column exists) and remove rows with missing 'word'\n",
    "        if 'word' in df.columns:\n",
    "            # Check if other language identifier columns also exist\n",
    "            if 'English (en)' in df.columns:\n",
    "                df = df[df['English (en)'] == 1]  # Keep only English words\n",
    "            df = df[df['word'].notna()]  # Drop rows with missing 'word' values\n",
    "        else:\n",
    "            print(\"Error: 'word' column not found after renaming.\")\n",
    "            return None\n",
    "\n",
    "        # 3. Select only the required columns.\n",
    "        required_columns = ['word', 'positive', 'negative', 'anger', 'anticipation',\n",
    "                         'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "        # Get a list of the columns present in the dataframe, from the required columns.\n",
    "        existing_columns = [col for col in required_columns if col in df.columns]\n",
    "\n",
    "        df = df[existing_columns]\n",
    "\n",
    "\n",
    "\n",
    "        return df  # Return the DataFrame\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, tokenize, remove stop words and punctuation, and lemmatize.\"\"\"\n",
    "    try:\n",
    "        # Explicitly load the Punkt sentence tokenizer\n",
    "        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        # Tokenize into sentences, *then* into words.\n",
    "        sentences = sent_tokenizer.tokenize(text.lower())\n",
    "        tokens = []\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent, language='english')  # Pass language here\n",
    "            tokens.extend(words)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return tokens\n",
    "\n",
    "    except LookupError as e:\n",
    "        print(f\"LookupError in preprocess_text: {e}\")\n",
    "        #Print helpful information:\n",
    "        print(f\"NLTK Data Path: {nltk.data.path}\")\n",
    "        import os\n",
    "        print(f\"NLTK_DATA environment variable: {os.environ.get('NLTK_DATA')}\")\n",
    "        print(f\"Does the punkt file exist where expected? {os.path.exists(nltk.data.find('tokenizers/punkt/PY3/english.pickle'))}\")\n",
    "        return []  # Return an empty list on error\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in preprocess_text: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def analyze_sentiment(text, lexicon_df):\n",
    "    \"\"\"Analyzes the sentiment of a text using the loaded lexicon data.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        lexicon_df (pd.DataFrame): The emotion lexicon DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of emotion scores for the text.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    emotion_scores = {\n",
    "        'positive': 0,\n",
    "        'negative': 0,\n",
    "        'anger': 0,\n",
    "        'anticipation': 0,\n",
    "        'disgust': 0,\n",
    "        'fear': 0,\n",
    "        'joy': 0,\n",
    "        'sadness': 0,\n",
    "        'surprise': 0,\n",
    "        'trust': 0\n",
    "    }\n",
    "\n",
    "    if lexicon_df.empty:\n",
    "        print(\"Warning: Lexicon is empty. Returning zero scores.\")\n",
    "        return emotion_scores\n",
    "\n",
    "    for word in tokens:\n",
    "        # Find *all* rows matching the word (case-insensitive)\n",
    "        # Convert the word column to lowercase for matching, and match word with lowercase.\n",
    "        matching_rows = lexicon_df[lexicon_df['word'].str.lower() == word.lower()]\n",
    "\n",
    "        for _, row in matching_rows.iterrows():  # Iterate through matching rows\n",
    "            for emotion in emotion_scores.keys():\n",
    "                # Get the emotion score directly from the DataFrame\n",
    "                emotion_scores[emotion] += int(row[emotion])  # Ensure it's an integer\n",
    "\n",
    "    return emotion_scores\n",
    "\n",
    "def expand_lexicon(lexicon_df):\n",
    "    \"\"\"Expands the lexicon DataFrame with synonyms from WordNet.\"\"\"\n",
    "    new_rows = []  # List to store new rows\n",
    "\n",
    "    for _, row in lexicon_df.iterrows():  # Iterate over rows directly\n",
    "        word = row['word']\n",
    "        # Check if the word is a string\n",
    "        if isinstance(word, str):\n",
    "            emotions = row.drop('word').to_dict() #drop word, to iterate over emotions\n",
    "            for synset in wordnet.synsets(word):\n",
    "                for lemma in synset.lemmas():\n",
    "                    lemma_name = lemma.name().replace(\"_\", \" \")  # Clean up lemma name\n",
    "                    # Check if the lemma already exists (case-insensitive)\n",
    "                    if lemma_name.lower() not in lexicon_df['word'].str.lower().values:\n",
    "                      new_row = {'word': lemma_name}\n",
    "                      new_row.update(emotions) # add all emotions columns\n",
    "                      new_rows.append(new_row) #add to the new rows\n",
    "\n",
    "    # Create a DataFrame from the new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    # Concatenate the original DataFrame with the new rows, ignore original index.\n",
    "    expanded_lexicon_df = pd.concat([lexicon_df, new_df], ignore_index=True)\n",
    "    expanded_lexicon_df = expanded_lexicon_df.drop_duplicates(subset=['word'], keep='first')\n",
    "    return expanded_lexicon_df\n",
    "\n",
    "# --- Main Program ---\n",
    "\n",
    "# Local file path (USE THIS FOR NOW)\n",
    "file_path = r\"C:\\Users\\menno\\Source\\Repos\\ML Emotions\\NRC-Emotion-Lexicon-v0.92-InManyLanguages-web.xlsx\"\n",
    "emotion_lexicon_df = load_nrc_lexicon(file_path=file_path)\n",
    "\n",
    "if emotion_lexicon_df is not None:\n",
    "    # LIMIT TO A SAMPLE FOR TESTING\n",
    "    sample_lexicon_df = emotion_lexicon_df.head(100)  # First 100 rows\n",
    "    #sample_lexicon_df = emotion_lexicon_df.sample(n=100) # you can use random sample instead of head.\n",
    "\n",
    "    print(f\"Original Lexicon Size: {len(emotion_lexicon_df)}\")\n",
    "    print(f\"Sample Lexicon Size: {len(sample_lexicon_df)}\")\n",
    "\n",
    "    expanded_lexicon_df = expand_lexicon(sample_lexicon_df)  # Use the SAMPLE\n",
    "    print(f\"Expanded Lexicon Size: {len(expanded_lexicon_df)}\")\n",
    "\n",
    "    # Example Usage (using the sample)\n",
    "    text1 = \"This is a wonderfully happy and joyful day!\"\n",
    "    text2 = \"I am feeling sad, angry, and filled with fear.\"\n",
    "    text3 = \"The movie was okay.  It wasn't amazing, but not terrible.\"\n",
    "\n",
    "    scores1 = analyze_sentiment(text1, expanded_lexicon_df)  # Use expanded lexicon\n",
    "    scores2 = analyze_sentiment(text2, expanded_lexicon_df)\n",
    "    scores3 = analyze_sentiment(text3, expanded_lexicon_df)\n",
    "\n",
    "    print(f\"Text 1 Scores: {scores1}\")\n",
    "    print(f\"Text 2 Scores: {scores2}\")\n",
    "    print(f\"Text 3 Scores: {scores3}\")\n",
    "\n",
    "    print(emotion_lexicon_df[emotion_lexicon_df['word'] == 'grief'])\n",
    "    print(expanded_lexicon_df[expanded_lexicon_df['word'] == 'sorrow'])\n",
    "\n",
    "else:\n",
    "    print(\"Failed to load emotion lexicon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheets: ['NRC-Emotion-Lexicon-v0.92-InMan']\n",
      "Using sheet: NRC-Emotion-Lexicon-v0.92-InMan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\menno\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Lexicon Size: 14181\n",
      "Expanded Lexicon Size: 35348\n",
      "Expanded lexicon saved to expanded_nrc_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "# ... (All your import statements) ...\n",
    "\n",
    "# ... (Your load_nrc_lexicon function - as in the previous, corrected code) ...\n",
    "# ... (Your expand_lexicon function - as in the previous, corrected code) ...\n",
    "# NO NEED TO REDEFINE THESE FUNCTIONS EVERY TIME\n",
    "\n",
    "# --- Lexicon Creation (Run ONCE) ---\n",
    "#\n",
    "# Load the original lexicon from your file.  Use *either* the URL\n",
    "# or the file_path, but NOT both at the same time.\n",
    "#\n",
    "# FOR URL (if you get the server issues sorted out):\n",
    "# emotion_lexicon_df = load_nrc_lexicon(url=\"https://saifmohammad.com/WebDocs/NRC-Emotion-Lexicon-v0.92-InManyLanguages-web.xlsx\")\n",
    "\n",
    "# FOR LOCAL FILE (use this for now):\n",
    "emotion_lexicon_df = load_nrc_lexicon(file_path=r\"C:\\Users\\menno\\Source\\Repos\\ML Emotions\\NRC-Emotion-Lexicon-v0.92-InManyLanguages-web.xlsx\")\n",
    "\n",
    "if emotion_lexicon_df is not None:\n",
    "    print(f\"Original Lexicon Size: {len(emotion_lexicon_df)}\")\n",
    "\n",
    "    # Expand with synonyms (this is the slow part)\n",
    "    expanded_lexicon_df = expand_lexicon(emotion_lexicon_df)\n",
    "    print(f\"Expanded Lexicon Size: {len(expanded_lexicon_df)}\")\n",
    "\n",
    "    # Save the expanded lexicon to a CSV file\n",
    "    expanded_lexicon_df.to_csv(\"expanded_nrc_lexicon.csv\", index=False)\n",
    "    print(\"Expanded lexicon saved to expanded_nrc_lexicon.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to load the emotion lexicon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 Scores: {'positive': 3, 'negative': 0, 'anger': 0, 'anticipation': 1, 'disgust': 0, 'fear': 0, 'joy': 3, 'sadness': 0, 'surprise': 1, 'trust': 2}\n",
      "Text 2 Scores: {'positive': 1, 'negative': 4, 'anger': 4, 'anticipation': 1, 'disgust': 3, 'fear': 3, 'joy': 1, 'sadness': 2, 'surprise': 1, 'trust': 1}\n",
      "Text 3 Scores: {'positive': 1, 'negative': 2, 'anger': 2, 'anticipation': 0, 'disgust': 2, 'fear': 2, 'joy': 1, 'sadness': 2, 'surprise': 0, 'trust': 1}\n",
      "Text 4 Scores: {'positive': 5, 'negative': 1, 'anger': 0, 'anticipation': 2, 'disgust': 0, 'fear': 2, 'joy': 4, 'sadness': 0, 'surprise': 3, 'trust': 0}\n",
      "Text 5 Scores: {'positive': 1, 'negative': 3, 'anger': 3, 'anticipation': 0, 'disgust': 1, 'fear': 1, 'joy': 1, 'sadness': 1, 'surprise': 0, 'trust': 1}\n",
      "Text 6 Scores: {'positive': 2, 'negative': 2, 'anger': 1, 'anticipation': 2, 'disgust': 0, 'fear': 2, 'joy': 0, 'sadness': 1, 'surprise': 0, 'trust': 0}\n",
      "       word  positive  negative  anger  anticipation  disgust  fear  joy  \\\n",
      "5627  grief         0         1      0             0        0     0    0   \n",
      "\n",
      "      sadness  surprise  trust  \n",
      "5627        1         0      0  \n",
      "         word  positive  negative  anger  anticipation  disgust  fear  joy  \\\n",
      "11744  sorrow         0         1      0             0        0     1    0   \n",
      "\n",
      "       sadness  surprise  trust  \n",
      "11744        1         0      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# --- Function Definitions (Keep these) ---\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, tokenize, remove stop words and punctuation, and lemmatize.\"\"\"\n",
    "    try:\n",
    "        # Explicitly load the Punkt sentence tokenizer\n",
    "        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        # Tokenize into sentences, *then* into words.  PASS THE TOKENIZER!\n",
    "        sentences = sent_tokenizer.tokenize(text.lower())\n",
    "        tokens = []\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent, language='english')  # Pass language here\n",
    "            tokens.extend(words)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return tokens\n",
    "\n",
    "    except LookupError as e:\n",
    "        print(f\"LookupError in preprocess_text: {e}\")\n",
    "        #Print helpful information:\n",
    "        print(f\"NLTK Data Path: {nltk.data.path}\")\n",
    "        import os\n",
    "        print(f\"NLTK_DATA environment variable: {os.environ.get('NLTK_DATA')}\")\n",
    "        print(f\"Does the punkt file exist where expected? {os.path.exists(nltk.data.find('tokenizers/punkt/PY3/english.pickle'))}\")\n",
    "        return []  # Return an empty list on error\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in preprocess_text: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def analyze_sentiment(text, lexicon_df):\n",
    "    \"\"\"Analyzes the sentiment of a text using the loaded lexicon data.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        lexicon_df (pd.DataFrame): The emotion lexicon DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of emotion scores for the text.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    emotion_scores = {\n",
    "        'positive': 0,\n",
    "        'negative': 0,\n",
    "        'anger': 0,\n",
    "        'anticipation': 0,\n",
    "        'disgust': 0,\n",
    "        'fear': 0,\n",
    "        'joy': 0,\n",
    "        'sadness': 0,\n",
    "        'surprise': 0,\n",
    "        'trust': 0\n",
    "    }\n",
    "\n",
    "    if lexicon_df.empty:\n",
    "        print(\"Warning: Lexicon is empty. Returning zero scores.\")\n",
    "        return emotion_scores\n",
    "\n",
    "    for word in tokens:\n",
    "        # Case-insensitive check if the word exists in the lexicon\n",
    "        if word.lower() in lexicon_df['word'].str.lower().values:\n",
    "            word_row = lexicon_df[lexicon_df['word'].str.lower() == word.lower()].iloc[0]\n",
    "            for emotion in emotion_scores.keys():\n",
    "                emotion_scores[emotion] += int(word_row[emotion])\n",
    "\n",
    "    return emotion_scores\n",
    "# --- Main Program: Sentiment Analysis ---\n",
    "\n",
    "# Load the *expanded* lexicon from the CSV file\n",
    "expanded_lexicon_df = pd.read_csv(\"expanded_nrc_lexicon.csv\")  # Load the SAVED lexicon\n",
    "\n",
    "# Example Usage (using the sample)\n",
    "text1 = \"This is a wonderfully happy and joyful day!\"\n",
    "text2 = \"I am feeling sad, angry, and filled with fear.\"\n",
    "text3 = \"The movie was okay.  It wasn't amazing, but not terrible.\"\n",
    "text4 = \"The unexpected gift filled me with joy and surprise! I was so grateful.\"\n",
    "text5 = \"He felt abandoned and betrayed by his closest friends.  The injustice of it all made him furious.\"\n",
    "text6 = \"The looming deadline and the overwhelming workload created a sense of dread and anxiety.\"\n",
    "\n",
    "scores1 = analyze_sentiment(text1, expanded_lexicon_df)  # Use expanded lexicon\n",
    "scores2 = analyze_sentiment(text2, expanded_lexicon_df)\n",
    "scores3 = analyze_sentiment(text3, expanded_lexicon_df)\n",
    "scores4 = analyze_sentiment(text4, expanded_lexicon_df)\n",
    "scores5 = analyze_sentiment(text5, expanded_lexicon_df)\n",
    "scores6 = analyze_sentiment(text6, expanded_lexicon_df)\n",
    "\n",
    "\n",
    "print(f\"Text 1 Scores: {scores1}\")\n",
    "print(f\"Text 2 Scores: {scores2}\")\n",
    "print(f\"Text 3 Scores: {scores3}\")\n",
    "print(f\"Text 4 Scores: {scores4}\")\n",
    "print(f\"Text 5 Scores: {scores5}\")\n",
    "print(f\"Text 6 Scores: {scores6}\")\n",
    "\n",
    "\n",
    "print(emotion_lexicon_df[emotion_lexicon_df['word'] == 'grief'])\n",
    "print(expanded_lexicon_df[expanded_lexicon_df['word'] == 'sorrow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plan to do that, combining best practices for both version control and continued development:\n",
    "\n",
    "1. Save Point (Working Code - Local File Loading):\n",
    "\n",
    "This is the complete, working code that loads the lexicon from a local Excel file, expands it with synonyms, and performs basic sentiment analysis with negation handling. This is your stable baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def load_nrc_lexicon(file_path=None):\n",
    "    \"\"\"Loads the NRC Emotion Lexicon from a local file and returns a DataFrame.\n",
    "       Handles both XLSX and TSV/CSV formats.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        raise ValueError(\"Must provide a file path.\")\n",
    "\n",
    "    try:\n",
    "        # Load from a local file\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            print(\"Available sheets:\", excel_file.sheet_names)\n",
    "            sheet_name = excel_file.sheet_names[0] # Get the *actual* first sheet name\n",
    "            print(f\"Using sheet: {sheet_name}\")\n",
    "            df = excel_file.parse(sheet_name)\n",
    "        elif file_path.endswith('.txt') or file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath_or_buffer=file_path,sep='\\t', header=0) # Added header=0 for CSV with header row\n",
    "\n",
    "            if len(df.columns) < 11:\n",
    "              print(f\"Warning the file has an invalid amount of columns: {len(df.columns)} expected at least 11\")\n",
    "              return None\n",
    "        else:\n",
    "            print(\"Unsupported file format.  Please provide a .xlsx or .tsv/.csv file.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        # --- Data Cleaning and Preparation ---\n",
    "\n",
    "        # 1. Handle inconsistent column names (KEY FIX)\n",
    "        # We'll rename the columns to a consistent set, *regardless* of\n",
    "        # whether they have spaces, parentheses, etc.\n",
    "        column_mapping = {\n",
    "            'English (en)': 'word',\n",
    "            'English': 'word',  # Handle case where it's just 'English'\n",
    "            'English Word': 'word',  # ***CORRECT MAPPING***\n",
    "             # Add other variations if needed, based on Step 1 output\n",
    "            'Positive': 'positive',\n",
    "            'Negative': 'negative',\n",
    "            'Anger': 'anger',\n",
    "            'Anticipation': 'anticipation',\n",
    "            'Disgust': 'disgust',\n",
    "            'Fear': 'fear',\n",
    "            'Joy': 'joy',\n",
    "            'Sadness': 'sadness',\n",
    "            'Surprise': 'surprise',\n",
    "            'Trust': 'trust'\n",
    "        }\n",
    "\n",
    "        # Rename columns, only if they exist in the DataFrame\n",
    "        for original, new in column_mapping.items():\n",
    "            if original in df.columns:\n",
    "                df = df.rename(columns={original: new})\n",
    "\n",
    "        # 2. Filter for English Words (if the column exists) and remove rows with missing 'word'\n",
    "        if 'word' in df.columns:\n",
    "            # Check if other language identifier columns also exist\n",
    "            if 'English (en)' in df.columns:\n",
    "                df = df[df['English (en)'] == 1]  # Keep only English words\n",
    "            df = df[df['word'].notna()]  # Drop rows with missing 'word' values\n",
    "        else:\n",
    "            print(\"Error: 'word' column not found after renaming.\")\n",
    "            return None\n",
    "\n",
    "        # 3. Select only the required columns.\n",
    "        required_columns = ['word', 'positive', 'negative', 'anger', 'anticipation',\n",
    "                         'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "        # Get a list of the columns present in the dataframe, from the required columns.\n",
    "        existing_columns = [col for col in required_columns if col in df.columns]\n",
    "\n",
    "        df = df[existing_columns]\n",
    "\n",
    "\n",
    "\n",
    "        return df  # Return the DataFrame\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, tokenize, remove stop words and punctuation, and lemmatize.\"\"\"\n",
    "    try:\n",
    "        # Explicitly load the Punkt sentence tokenizer\n",
    "        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        # Tokenize into sentences, *then* into words.\n",
    "        sentences = sent_tokenizer.tokenize(text.lower())\n",
    "        tokens = []\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent, language='english')  # Pass language here\n",
    "            tokens.extend(words)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return tokens\n",
    "\n",
    "    except LookupError as e:\n",
    "        print(f\"LookupError in preprocess_text: {e}\")\n",
    "        #Print helpful information:\n",
    "        print(f\"NLTK Data Path: {nltk.data.path}\")\n",
    "        import os\n",
    "        print(f\"NLTK_DATA environment variable: {os.environ.get('NLTK_DATA')}\")\n",
    "        print(f\"Does the punkt file exist where expected? {os.path.exists(nltk.data.find('tokenizers/punkt/PY3/english.pickle'))}\")\n",
    "        return []  # Return an empty list on error\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in preprocess_text: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def analyze_sentiment(text, lexicon_df):\n",
    "    \"\"\"Analyzes the sentiment of a text using the loaded lexicon data.\"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    print(f\"Final tokens for analysis: {tokens}\")  # Debugging print\n",
    "\n",
    "    emotion_scores = {\n",
    "        'positive': 0,\n",
    "        'negative': 0,\n",
    "        'anger': 0,\n",
    "        'anticipation': 0,\n",
    "        'disgust': 0,\n",
    "        'fear': 0,\n",
    "        'joy': 0,\n",
    "        'sadness': 0,\n",
    "        'surprise': 0,\n",
    "        'trust': 0\n",
    "    }\n",
    "\n",
    "    if lexicon_df.empty:\n",
    "        print(\"Warning: Lexicon is empty. Returning zero scores.\")\n",
    "        return emotion_scores\n",
    "\n",
    "    for word in tokens:\n",
    "        # ***CRITICAL FIX: Lowercase BOTH the word and the lexicon words***\n",
    "        word_lower = word.lower()\n",
    "        matching_rows = lexicon_df[lexicon_df['word'].str.lower() == word_lower]\n",
    "\n",
    "        # Iterate through the matching row(s) and sum the scores\n",
    "        for _, row in matching_rows.iterrows():\n",
    "            for emotion in emotion_scores.keys():\n",
    "                try:\n",
    "                    emotion_scores[emotion] += int(row[emotion])\n",
    "                except KeyError as e:\n",
    "                  print(f\"KeyError: {e} not found in DataFrame. Check your column names!\")\n",
    "                  return {} #return empty dictionary on error.\n",
    "                except ValueError as e:\n",
    "                  print(\"ValueError\")\n",
    "\n",
    "    return emotion_scores\n",
    "\n",
    "def expand_lexicon(lexicon_df):\n",
    "    \"\"\"Expands the lexicon DataFrame with synonyms from WordNet.\"\"\"\n",
    "    new_rows = []  # List to store new rows\n",
    "\n",
    "    for _, row in lexicon_df.iterrows():  # Iterate over rows directly\n",
    "        word = row['word']\n",
    "        # Check if the word is a string\n",
    "        if isinstance(word, str):\n",
    "            emotions = row.drop('word').to_dict() #drop word, to iterate over emotions\n",
    "            for synset in wordnet.synsets(word):\n",
    "                for lemma in synset.lemmas():\n",
    "                    lemma_name = lemma.name().replace(\"_\", \" \")  # Clean up lemma name\n",
    "                    # Check if the lemma already exists (case-insensitive)\n",
    "                    if lemma_name.lower() not in lexicon_df['word'].str.lower().values:\n",
    "                      new_row = {'word': lemma_name}\n",
    "                      new_row.update(emotions) # add all emotions columns\n",
    "                      new_rows.append(new_row) #add to the new rows\n",
    "\n",
    "    # Create a DataFrame from the new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    # Concatenate the original DataFrame with the new rows, ignore original index.\n",
    "    expanded_lexicon_df = pd.concat([lexicon_df, new_df], ignore_index=True)\n",
    "    expanded_lexicon_df = expanded_lexicon_df.drop_duplicates(subset=['word'], keep='first')\n",
    "    return expanded_lexicon_df\n",
    "\n",
    "# --- Main Program ---\n",
    "\n",
    "# 1. Load Lexicon (from local file within Fabric)\n",
    "file_path = r\"C:\\Users\\menno\\Source\\Repos\\ML Emotions\\NRC-Emotion-Lexicon-v0.92-InManyLanguages-web.xlsx\"  # Correct path within \"Files\"\n",
    "emotion_lexicon_df = load_nrc_lexicon(file_path=file_path)\n",
    "\n",
    "if emotion_lexicon_df is not None:\n",
    "    # 2. Limit for Testing (Optional - remove for full processing)\n",
    "    #sample_lexicon_df = emotion_lexicon_df.head(100)  # First 100 rows #Removed to use full dataset\n",
    "    #sample_lexicon_df = emotion_lexicon_df.sample(n=100) # you can use random sample instead of head.\n",
    "\n",
    "    print(f\"Original Lexicon Size: {len(emotion_lexicon_df)}\")\n",
    "    #print(f\"Sample Lexicon Size: {len(sample_lexicon_df)}\") #Removed sample size\n",
    "\n",
    "    # 3. Expand Lexicon (Optional - you can skip this initially for even faster testing)\n",
    "    expanded_lexicon_df = expand_lexicon(emotion_lexicon_df)  # Use the SAMPLE\n",
    "    print(f\"Expanded Lexicon Size: {len(expanded_lexicon_df)}\")\n",
    "\n",
    "    # 4. Example Usage (using the *expanded* lexicon)\n",
    "    text1 = \"This is a wonderfully happy and joyful day!\"\n",
    "    text2 = \"I am feeling sad, angry, and filled with fear.\"\n",
    "    text3 = \"The movie was okay.  It wasn't amazing, but not terrible.\"\n",
    "    text4 = \"I can't believe I got the promotion, but now I have so much more responsibility.\"\n",
    "    text5 = \"The movie was incredibly well-made, but it left me feeling empty inside.\"\n",
    "    text6 = \"She said she loves me, but I don't know if I can trust her anymore.\"\n",
    "    text7 = \"Winning the lottery was the best thing that ever happened to me, but now everyone wants a piece of my fortune.\"\n",
    "    text8 = \"I finally finished the marathon, but my legs feel like they're going to fall off.\"\n",
    "    text9 = \"The concert was amazing, but the crowd was overwhelming.\"\n",
    "    text10 = \"I got an A on my exam, but I feel like I didn't really learn anything.\"\n",
    "    text11 = \"He apologized for his mistake, but I still feel hurt.\"\n",
    "    text12 = \"The new job pays well, but I miss my old colleagues.\"\n",
    "    text13 = \"I love my new house, but the neighborhood is a bit too quiet for my liking.\"\n",
    "    text14 = \"I am thrilled to announce that I got the job!\"\n",
    "    text15 = \"I am devastated by the loss of my pet.\"\n",
    "    text16 = \"I am so proud of my daughter's achievements.\"\n",
    "    text17 = \"I am furious about the unfair treatment I received.\"\n",
    "    text18 = \"I am anxious about the upcoming exam.\"\n",
    "    text19 = \"I am grateful for all the support from my friends.\"\n",
    "    text20 = \"I am disgusted by the behavior I witnessed.\"\n",
    "    text21 = \"I am excited to travel to a new country.\"\n",
    "    text22 = \"I am relieved that the surgery went well.\"\n",
    "    text23 = \"I am disappointed with the results of the project.\"\n",
    "\n",
    "    scores1 = analyze_sentiment(text1, expanded_lexicon_df)  # Use expanded lexicon\n",
    "    scores2 = analyze_sentiment(text2, expanded_lexicon_df)\n",
    "    scores3 = analyze_sentiment(text3, expanded_lexicon_df)\n",
    "    scores4 = analyze_sentiment(text4, expanded_lexicon_df)\n",
    "    scores5 = analyze_sentiment(text5, expanded_lexicon_df)\n",
    "    scores6 = analyze_sentiment(text6, expanded_lexicon_df)\n",
    "    scores7 = analyze_sentiment(text7, expanded_lexicon_df)\n",
    "    scores8 = analyze_sentiment(text8, expanded_lexicon_df)\n",
    "    scores9 = analyze_sentiment(text9, expanded_lexicon_df)\n",
    "    scores10 = analyze_sentiment(text10, expanded_lexicon_df)\n",
    "    scores11 = analyze_sentiment(text11, expanded_lexicon_df)\n",
    "    scores12 = analyze_sentiment(text12, expanded_lexicon_df)\n",
    "    scores13 = analyze_sentiment(text13, expanded_lexicon_df)\n",
    "    scores14 = analyze_sentiment(text14, expanded_lexicon_df)\n",
    "    scores15 = analyze_sentiment(text15, expanded_lexicon_df)\n",
    "    scores16 = analyze_sentiment(text16, expanded_lexicon_df)\n",
    "    scores17 = analyze_sentiment(text17, expanded_lexicon_df)\n",
    "    scores18 = analyze_sentiment(text18, expanded_lexicon_df)\n",
    "    scores19 = analyze_sentiment(text19, expanded_lexicon_df)\n",
    "    scores20 = analyze_sentiment(text20, expanded_lexicon_df)\n",
    "    scores21 = analyze_sentiment(text21, expanded_lexicon_df)\n",
    "    scores22 = analyze_sentiment(text22, expanded_lexicon_df)\n",
    "    scores23 = analyze_sentiment(text23, expanded_lexicon_df)\n",
    "    print(f\"Text 1 Scores: {scores1}\")\n",
    "    print(f\"Text 2 Scores: {scores2}\")\n",
    "    print(f\"Text 3 Scores: {scores3}\")\n",
    "    print(f\"Text 4 Scores: {scores4}\")\n",
    "    print(f\"Text 5 Scores: {scores5}\")\n",
    "    print(f\"Text 6 Scores: {scores6}\")\n",
    "    print(f\"Text 7 Scores: {scores7}\")\n",
    "    print(f\"Text 8 Scores: {scores8}\")\n",
    "    print(f\"Text 9 Scores: {scores9}\")\n",
    "    print(f\"Text 10 Scores: {scores10}\")\n",
    "    print(f\"Text 11 Scores: {scores11}\")\n",
    "    print(f\"Text 12 Scores: {scores12}\")\n",
    "    print(f\"Text 13 Scores: {scores13}\")\n",
    "    print(f\"Text 14 Scores: {scores14}\")\n",
    "    print(f\"Text 15 Scores: {scores15}\")\n",
    "    print(f\"Text 16 Scores: {scores16}\")\n",
    "    print(f\"Text 17 Scores: {scores17}\")\n",
    "    print(f\"Text 18 Scores: {scores18}\")\n",
    "    print(f\"Text 19 Scores: {scores19}\")\n",
    "    print(f\"Text 20 Scores: {scores20}\")\n",
    "    print(f\"Text 21 Scores: {scores21}\")\n",
    "    print(f\"Text 22 Scores: {scores22}\")\n",
    "    print(f\"Text 23 Scores: {scores23}\")\n",
    "\n",
    "    print(emotion_lexicon_df[emotion_lexicon_df['word'] == 'grief'])\n",
    "    print(expanded_lexicon_df[expanded_lexicon_df['word'] == 'sorrow'])\n",
    "\n",
    "else:\n",
    "    print(\"Failed to load emotion lexicon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "Run the code.\n",
    "\n",
    "Add a line to save the expanded lexicon to a csv file: expanded_lexicon_df.to_csv(\"expanded_nrc_lexicon.csv\", index=False)\n",
    "\n",
    "Change the file loading to use this new file.\n",
    "\n",
    "Summary for Next Prompt:\n",
    "\n",
    "To move this project forward and start a new, focused prompt, here's a summary of where we are and what the next logical steps would be:\n",
    "\n",
    "Current Status:\n",
    "\n",
    "Working Baseline: We have a functional Python script that:\n",
    "\n",
    "Loads the NRC Emotion Lexicon from a local Excel file.\n",
    "\n",
    "Preprocesses text data (lowercasing, tokenization, stop word removal, punctuation removal, lemmatization).\n",
    "\n",
    "Expands the lexicon with synonyms using WordNet.\n",
    "\n",
    "Calculates basic emotion scores for input text by summing the emotion scores of matching words (and their synonyms) in the lexicon.\n",
    "\n",
    "Includes basic negation handling (inverting sentiment for words following negation terms).\n",
    "\n",
    "Runs in a Fabric PySpark notebook environment (though we are not yet fully leveraging Spark's distributed processing capabilities).\n",
    "\n",
    "Includes basic error handling.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Simple Sentiment Scoring: The current scoring method is a simple sum of emotion scores. This doesn't account for word frequency, sentence structure, or more complex linguistic phenomena.\n",
    "\n",
    "Basic Negation: Negation handling is rudimentary, only affecting the immediately following word.\n",
    "\n",
    "No Contextual Understanding: The system treats words in isolation, without considering their context. This leads to inaccuracies with words that have multiple meanings (polysemy) or where sentiment is expressed indirectly (sarcasm, irony).\n",
    "\n",
    "Pandas DataFrames (Not Fully Spark): We are loading and processing the lexicon using Pandas DataFrames, which are not distributed. While this works for the current lexicon size, it won't scale efficiently to very large text datasets or lexicons. We have a working baseline, then we can move to a fully Spark-based solution.\n",
    "\n",
    "Next Steps (for a New Prompt - Prioritized):\n",
    "\n",
    "Here's a prioritized list of next steps, suitable for framing a new prompt:\n",
    "\n",
    "Improved Negation Handling:\n",
    "\n",
    "Goal: More accurately handle negation to avoid misinterpreting the sentiment of phrases like \"not happy\" or \"didn't enjoy.\"\n",
    "\n",
    "Methods:\n",
    "\n",
    "Wider Negation Window: Extend the negation effect to more than just the immediately following word (e.g., a window of 2-3 words).\n",
    "\n",
    "Dependency Parsing: Use a dependency parser (like spaCy) to identify the grammatical relationships between words. This is much more accurate than a fixed window. For example, in \"I did not find the movie enjoyable,\" a dependency parser would correctly link \"not\" to \"enjoyable,\" even though they are not adjacent.\n",
    "\n",
    "Example Prompt: \"How can I improve the negation handling in my Python sentiment analysis code? I'm currently using a simple flag to invert the sentiment of the next word after a negation word (like 'not'), but this isn't accurate enough. I'd like to explore using a wider window, and ideally, I'd like to use dependency parsing with spaCy to identify the words being negated more accurately. Provide code examples using spaCy, and explain how to integrate it into my existing preprocess_text and analyze_sentiment functions.\"\n",
    "\n",
    "Contextual Word Embeddings (BERT):\n",
    "\n",
    "Goal: Move beyond simple word matching and capture the meaning of words in context.\n",
    "\n",
    "Method: Use a pre-trained BERT model (or similar transformer-based model) to generate contextual word embeddings.\n",
    "\n",
    "Example Prompt: \"I want to improve my sentiment analysis by using contextual word embeddings. I've heard that BERT is a good choice. How can I integrate a pre-trained BERT model into my existing Python code to generate word embeddings, and how would I use those embeddings to calculate sentiment scores? Provide a code example that shows how to load a BERT model (using the transformers library), get embeddings for words in a sentence, and then use those embeddings, along with my existing emotion lexicon, to calculate sentiment. I want the approach to be compatible with eventual use in a PySpark environment.\"\n",
    "\n",
    "Weighted Scoring and Normalization:\n",
    "\n",
    "Goal: Improve the scoring mechanism to be more nuanced than a simple sum.\n",
    "\n",
    "Methods:\n",
    "\n",
    "TF-IDF: Weight words by their Term Frequency-Inverse Document Frequency (TF-IDF). This gives more weight to words that are frequent in a document but relatively rare in the overall corpus.\n",
    "\n",
    "Normalization: Divide the emotion scores by the total number of (processed) words in the text to account for different text lengths.\n",
    "\n",
    "Custom Weights: Experiment with assigning different weights to different emotion categories, or to specific words.\n",
    "\n",
    "Example Prompt: \"How can I improve the sentiment scoring in my Python code? Currently, I'm just summing the emotion scores from my lexicon. I'd like to explore weighting words by TF-IDF and normalizing the scores by the length of the text. Show me how to calculate TF-IDF scores for the words in my text and use those scores to weight the emotion scores from the lexicon.\"\n",
    "\n",
    "Converting fully to PySpark Dataframes\n",
    "\n",
    "This allows for larger files and datasets to be processed\n",
    "\n",
    "Make use of the full functionality of the Fabric environment.\n",
    "\n",
    "Machine Learning:\n",
    "\n",
    "Goal: Move beyond lexicon-based analysis to a more powerful, data-driven approach.\n",
    "\n",
    "Methods: Train a machine learning classifier (e.g., Naive Bayes, SVM, Random Forest, or a neural network) on a labeled dataset of text with known sentiment/emotion labels. Use the lexicon scores, word embeddings, and other features (n-grams, POS tags) as input to the classifier.\n",
    "\n",
    "Example prompt: What would be the best approach to use machine learning with the current code, so that the emotion of a text is detected.\n",
    "\n",
    "Choose one of these areas to focus on in your next prompt. Don't try to do everything at once. Start with negation handling, as that's a relatively self-contained improvement that will have a noticeable impact on accuracy. Then, you can move on to more advanced techniques like contextual embeddings and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [101 lines of output]\n",
      "      Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-76.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.9.tar.gz (14 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting thinc<8.4.0,>=8.3.0\n",
      "        Using cached thinc-8.3.2.tar.gz (193 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        Ã— pip subprocess to install build dependencies did not run successfully.\n",
      "        â”‚ exit code: 1\n",
      "        â•°â”€> [65 lines of output]\n",
      "            Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n",
      "            Collecting setuptools\n",
      "              Using cached setuptools-76.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "            Collecting cython<3.0,>=0.25\n",
      "              Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "            Collecting murmurhash<1.1.0,>=1.0.2\n",
      "              Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "            Collecting cymem<2.1.0,>=2.0.2\n",
      "              Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "            Collecting preshed<3.1.0,>=3.0.2\n",
      "              Using cached preshed-3.0.9.tar.gz (14 kB)\n",
      "              Installing build dependencies: started\n",
      "              Installing build dependencies: finished with status 'done'\n",
      "              Getting requirements to build wheel: started\n",
      "              Getting requirements to build wheel: finished with status 'done'\n",
      "              Preparing metadata (pyproject.toml): started\n",
      "              Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "            Collecting blis<1.1.0,>=1.0.0\n",
      "              Using cached blis-1.0.2-cp313-cp313-win_amd64.whl.metadata (7.8 kB)\n",
      "            Collecting numpy<2.1.0,>=2.0.0\n",
      "              Using cached numpy-2.0.2.tar.gz (18.9 MB)\n",
      "              Installing build dependencies: started\n",
      "              Installing build dependencies: finished with status 'done'\n",
      "              Getting requirements to build wheel: started\n",
      "              Getting requirements to build wheel: finished with status 'done'\n",
      "              Installing backend dependencies: started\n",
      "              Installing backend dependencies: finished with status 'done'\n",
      "              Preparing metadata (pyproject.toml): started\n",
      "              Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "              error: subprocess-exited-with-error\n",
      "      \n",
      "              Ãƒâ€” Preparing metadata (pyproject.toml) did not run successfully.\n",
      "              Ã¢â€\\x9dâ€š exit code: 1\n",
      "              Ã¢â€¢Â°Ã¢â€\\x9dâ‚¬> [21 lines of output]\n",
      "                  + C:\\Users\\menno\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e\\vendored-meson\\meson\\meson.py setup C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e\\.mesonpy-_di8m621 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e\\.mesonpy-_di8m621\\meson-python-native-file.ini\n",
      "                  The Meson build system\n",
      "                  Version: 1.4.99\n",
      "                  Source dir: C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e\n",
      "                  Build dir: C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e\\.mesonpy-_di8m621\n",
      "                  Build type: native build\n",
      "                  Project name: NumPy\n",
      "                  Project version: 2.0.2\n",
      "                  WARNING: Failed to activate VS environment: Could not parse vswhere.exe output\n",
      "      \n",
      "                  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "                  The following exception(s) were encountered:\n",
      "                  Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "                  Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "                  Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "                  Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "                  Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "                  Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "                  Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "                  A full log can be found at C:\\Users\\menno\\AppData\\Local\\Temp\\pip-install-ple8a3ju\\numpy_076e6288208743f6a08fcf8966da3d2e\\.mesonpy-_di8m621\\meson-logs\\meson-log.txt\n",
      "                  [end of output]\n",
      "      \n",
      "              note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "            error: metadata-generation-failed\n",
      "      \n",
      "            Ãƒâ€” Encountered error while generating package metadata.\n",
      "            Ã¢â€¢Â°Ã¢â€\\x9dâ‚¬> See above for output.\n",
      "      \n",
      "            note: This is an issue with the package mentioned above, not pip.\n",
      "            hint: See above for details.\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: subprocess-exited-with-error\n",
      "      \n",
      "      Ã— pip subprocess to install build dependencies did not run successfully.\n",
      "      â”‚ exit code: 1\n",
      "      â•°â”€> See above for output.\n",
      "      \n",
      "      note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m  \u001b[38;5;66;03m# Import spaCy\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# --- Function Definitions ---\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_nrc_lexicon\u001b[39m(file_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import spacy  # Import spaCy\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def load_nrc_lexicon(file_path=None):\n",
    "    \"\"\"Loads the NRC Emotion Lexicon from a local file and returns a DataFrame.\n",
    "       Handles both XLSX and TSV/CSV formats.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        raise ValueError(\"Must provide a file path.\")\n",
    "\n",
    "    try:\n",
    "        # Load from a local file\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            print(\"Available sheets:\", excel_file.sheet_names)\n",
    "            sheet_name = excel_file.sheet_names[0] # Get the *actual* first sheet name\n",
    "            print(f\"Using sheet: {sheet_name}\")\n",
    "            df = excel_file.parse(sheet_name)\n",
    "        elif file_path.endswith('.txt') or file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath_or_buffer=file_path,sep='\\t', header=0) # Added header=0 for CSV with header row\n",
    "\n",
    "            if len(df.columns) < 11:\n",
    "              print(f\"Warning the file has an invalid amount of columns: {len(df.columns)} expected at least 11\")\n",
    "              return None\n",
    "        else:\n",
    "            print(\"Unsupported file format.  Please provide a .xlsx or .tsv/.csv file.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        # --- Data Cleaning and Preparation ---\n",
    "\n",
    "        # 1. Handle inconsistent column names (KEY FIX)\n",
    "        # We'll rename the columns to a consistent set, *regardless* of\n",
    "        # whether they have spaces, parentheses, etc.\n",
    "        column_mapping = {\n",
    "            'English (en)': 'word',\n",
    "            'English': 'word',  # Handle case where it's just 'English'\n",
    "            'English Word': 'word',  # ***CORRECT MAPPING***\n",
    "             # Add other variations if needed, based on Step 1 output\n",
    "            'Positive': 'positive',\n",
    "            'Negative': 'negative',\n",
    "            'Anger': 'anger',\n",
    "            'Anticipation': 'anticipation',\n",
    "            'Disgust': 'disgust',\n",
    "            'Fear': 'fear',\n",
    "            'Joy': 'joy',\n",
    "            'Sadness': 'sadness',\n",
    "            'Surprise': 'surprise',\n",
    "            'Trust': 'trust'\n",
    "        }\n",
    "\n",
    "        # Rename columns, only if they exist in the DataFrame\n",
    "        for original, new in column_mapping.items():\n",
    "            if original in df.columns:\n",
    "                df = df.rename(columns={original: new})\n",
    "\n",
    "        # 2. Filter for English Words (if the column exists) and remove rows with missing 'word'\n",
    "        if 'word' in df.columns:\n",
    "            # Check if other language identifier columns also exist\n",
    "            if 'English (en)' in df.columns:\n",
    "                df = df[df['English (en)'] == 1]  # Keep only English words\n",
    "            df = df[df['word'].notna()]  # Drop rows with missing 'word' values\n",
    "        else:\n",
    "            print(\"Error: 'word' column not found after renaming.\")\n",
    "            return None\n",
    "\n",
    "        # 3. Select only the required columns.\n",
    "        required_columns = ['word', 'positive', 'negative', 'anger', 'anticipation',\n",
    "                         'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "        # Get a list of the columns present in the dataframe, from the required columns.\n",
    "        existing_columns = [col for col in required_columns if col in df.columns]\n",
    "\n",
    "        df = df[existing_columns]\n",
    "\n",
    "\n",
    "\n",
    "        return df  # Return the DataFrame\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, tokenize, remove stop words and punctuation, and lemmatize.\"\"\"\n",
    "    try:\n",
    "        # Explicitly load the Punkt sentence tokenizer\n",
    "        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        # Tokenize into sentences, *then* into words.\n",
    "        sentences = sent_tokenizer.tokenize(text.lower())\n",
    "        tokens = []\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent, language='english')  # Pass language here\n",
    "            tokens.extend(words)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return tokens\n",
    "\n",
    "    except LookupError as e:\n",
    "        print(f\"LookupError in preprocess_text: {e}\")\n",
    "        #Print helpful information:\n",
    "        print(f\"NLTK Data Path: {nltk.data.path}\")\n",
    "        import os\n",
    "        print(f\"NLTK_DATA environment variable: {os.environ.get('NLTK_DATA')}\")\n",
    "        print(f\"Does the punkt file exist where expected? {os.path.exists(nltk.data.find('tokenizers/punkt/PY3/english.pickle'))}\")\n",
    "        return []  # Return an empty list on error\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in preprocess_text: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def analyze_sentiment(text, lexicon_df):\n",
    "    \"\"\"Analyzes the sentiment of a text using the loaded lexicon data.\"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    print(f\"Final tokens for analysis: {tokens}\")  # Debugging print\n",
    "\n",
    "    emotion_scores = {\n",
    "        'positive': 0,\n",
    "        'negative': 0,\n",
    "        'anger': 0,\n",
    "        'anticipation': 0,\n",
    "        'disgust': 0,\n",
    "        'fear': 0,\n",
    "        'joy': 0,\n",
    "        'sadness': 0,\n",
    "        'surprise': 0,\n",
    "        'trust': 0\n",
    "    }\n",
    "\n",
    "    if lexicon_df.empty:\n",
    "        print(\"Warning: Lexicon is empty. Returning zero scores.\")\n",
    "        return emotion_scores\n",
    "\n",
    "    for word in tokens:\n",
    "        # ***CRITICAL FIX: Lowercase BOTH the word and the lexicon words***\n",
    "        word_lower = word.lower()\n",
    "        matching_rows = lexicon_df[lexicon_df['word'].str.lower() == word_lower]\n",
    "\n",
    "        # Iterate through the matching row(s) and sum the scores\n",
    "        for _, row in matching_rows.iterrows():\n",
    "            for emotion in emotion_scores.keys():\n",
    "                try:\n",
    "                    emotion_scores[emotion] += int(row[emotion])\n",
    "                except KeyError as e:\n",
    "                  print(f\"KeyError: {e} not found in DataFrame. Check your column names!\")\n",
    "                  return {} #return empty dictionary on error.\n",
    "                except ValueError as e:\n",
    "                  print(\"ValueError\")\n",
    "\n",
    "    return emotion_scores\n",
    "\n",
    "def expand_lexicon(lexicon_df):\n",
    "    \"\"\"Expands the lexicon DataFrame with synonyms from WordNet.\"\"\"\n",
    "    new_rows = []  # List to store new rows\n",
    "\n",
    "    for _, row in lexicon_df.iterrows():  # Iterate over rows directly\n",
    "        word = row['word']\n",
    "        # Check if the word is a string\n",
    "        if isinstance(word, str):\n",
    "            emotions = row.drop('word').to_dict() #drop word, to iterate over emotions\n",
    "            for synset in wordnet.synsets(word):\n",
    "                for lemma in synset.lemmas():\n",
    "                    lemma_name = lemma.name().replace(\"_\", \" \")  # Clean up lemma name\n",
    "                    # Check if the lemma already exists (case-insensitive)\n",
    "                    if lemma_name.lower() not in lexicon_df['word'].str.lower().values:\n",
    "                      new_row = {'word': lemma_name}\n",
    "                      new_row.update(emotions) # add all emotions columns\n",
    "                      new_rows.append(new_row) #add to the new rows\n",
    "\n",
    "    # Create a DataFrame from the new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    # Concatenate the original DataFrame with the new rows, ignore original index.\n",
    "    expanded_lexicon_df = pd.concat([lexicon_df, new_df], ignore_index=True)\n",
    "    expanded_lexicon_df = expanded_lexicon_df.drop_duplicates(subset=['word'], keep='first')\n",
    "    return expanded_lexicon_df\n",
    "\n",
    "# --- Main Program ---\n",
    "# (Rest of your main program using the expanded lexicon)\n",
    "\n",
    "# Load the expanded lexicon\n",
    "expanded_lexicon_df = pd.read_csv(\"expanded_nrc_lexicon.csv\")\n",
    "\n",
    "text1 = \"This is a wonderfully happy and joyful day!\"\n",
    "text2 = \"I am feeling sad, angry, and filled with fear.\"\n",
    "text3 = \"The movie was okay.  It wasn't amazing, but not terrible.\"\n",
    "text4 = \"I can't believe I got the promotion, but now I have so much more responsibility.\"\n",
    "text5 = \"The movie was incredibly well-made, but it left me feeling empty inside.\"\n",
    "text6 = \"She said she loves me, but I don't know if I can trust her anymore.\"\n",
    "text7 = \"Winning the lottery was the best thing that ever happened to me, but now everyone wants a piece of my fortune.\"\n",
    "text8 = \"I finally finished the marathon, but my legs feel like they're going to fall off.\"\n",
    "text9 = \"The concert was amazing, but the crowd was overwhelming.\"\n",
    "text10 = \"I got an A on my exam, but I feel like I didn't really learn anything.\"\n",
    "text11 = \"He apologized for his mistake, but I still feel hurt.\"\n",
    "text12 = \"The new job pays well, but I miss my old colleagues.\"\n",
    "text13 = \"I love my new house, but the neighborhood is a bit too quiet for my liking.\"\n",
    "text14 = \"I am thrilled to announce that I got the job!\"\n",
    "text15 = \"I am devastated by the loss of my pet.\"\n",
    "text16 = \"I am so proud of my daughter's achievements.\"\n",
    "text17 = \"I am furious about the unfair treatment I received.\"\n",
    "text18 = \"I am anxious about the upcoming exam.\"\n",
    "text19 = \"I am grateful for all the support from my friends.\"\n",
    "text20 = \"I am disgusted by the behavior I witnessed.\"\n",
    "text21 = \"I am excited to travel to a new country.\"\n",
    "text22 = \"I am relieved that the surgery went well.\"\n",
    "text23 = \"I am disappointed with the results of the project.\"\n",
    "\n",
    "scores1 = analyze_sentiment(text1, expanded_lexicon_df)  # Use expanded lexicon\n",
    "scores2 = analyze_sentiment(text2, expanded_lexicon_df)\n",
    "scores3 = analyze_sentiment(text3, expanded_lexicon_df)\n",
    "scores4 = analyze_sentiment(text4, expanded_lexicon_df)\n",
    "scores5 = analyze_sentiment(text5, expanded_lexicon_df)\n",
    "scores6 = analyze_sentiment(text6, expanded_lexicon_df)\n",
    "scores7 = analyze_sentiment(text7, expanded_lexicon_df)\n",
    "scores8 = analyze_sentiment(text8, expanded_lexicon_df)\n",
    "scores9 = analyze_sentiment(text9, expanded_lexicon_df)\n",
    "scores10 = analyze_sentiment(text10, expanded_lexicon_df)\n",
    "scores11 = analyze_sentiment(text11, expanded_lexicon_df)\n",
    "scores12 = analyze_sentiment(text12, expanded_lexicon_df)\n",
    "scores13 = analyze_sentiment(text13, expanded_lexicon_df)\n",
    "scores14 = analyze_sentiment(text14, expanded_lexicon_df)\n",
    "scores15 = analyze_sentiment(text15, expanded_lexicon_df)\n",
    "scores16 = analyze_sentiment(text16, expanded_lexicon_df)\n",
    "scores17 = analyze_sentiment(text17, expanded_lexicon_df)\n",
    "scores18 = analyze_sentiment(text18, expanded_lexicon_df)\n",
    "scores19 = analyze_sentiment(text19, expanded_lexicon_df)\n",
    "scores20 = analyze_sentiment(text20, expanded_lexicon_df)\n",
    "scores21 = analyze_sentiment(text21, expanded_lexicon_df)\n",
    "scores22 = analyze_sentiment(text22, expanded_lexicon_df)\n",
    "scores23 = analyze_sentiment(text23, expanded_lexicon_df)\n",
    "print(f\"Text 1 Scores: {scores1}\")\n",
    "print(f\"Text 2 Scores: {scores2}\")\n",
    "print(f\"Text 3 Scores: {scores3}\")\n",
    "print(f\"Text 4 Scores: {scores4}\")\n",
    "print(f\"Text 5 Scores: {scores5}\")\n",
    "print(f\"Text 6 Scores: {scores6}\")\n",
    "print(f\"Text 7 Scores: {scores7}\")\n",
    "print(f\"Text 8 Scores: {scores8}\")\n",
    "print(f\"Text 9 Scores: {scores9}\")\n",
    "print(f\"Text 10 Scores: {scores10}\")\n",
    "print(f\"Text 11 Scores: {scores11}\")\n",
    "print(f\"Text 12 Scores: {scores12}\")\n",
    "print(f\"Text 13 Scores: {scores13}\")\n",
    "print(f\"Text 14 Scores: {scores14}\")\n",
    "print(f\"Text 15 Scores: {scores15}\")\n",
    "print(f\"Text 16 Scores: {scores16}\")\n",
    "print(f\"Text 17 Scores: {scores17}\")\n",
    "print(f\"Text 18 Scores: {scores18}\")\n",
    "print(f\"Text 19 Scores: {scores19}\")\n",
    "print(f\"Text 20 Scores: {scores20}\")\n",
    "print(f\"Text 21 Scores: {scores21}\")\n",
    "print(f\"Text 22 Scores: {scores22}\")\n",
    "print(f\"Text 23 Scores: {scores23}\")\n",
    "\n",
    "print(expanded_lexicon_df[expanded_lexicon_df['word'] == 'grief'])\n",
    "print(expanded_lexicon_df[expanded_lexicon_df['word'] == 'sorrow'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
